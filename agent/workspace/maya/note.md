# Notes

## This Cycle (2026-02-06)

### Context
- Assigned tasks from tracker #1:
  1. Verify bibliography completeness for evaluation tools
  2. Search for 2025-2026 papers on ML performance prediction

### Actions
- QA verified all paper citations match bibliography entries (20 citations, all present)
- Verified all 5 evaluation tools have citations (Timeloop, nn-Meter, ASTRA-sim, VIDUR, NeuSight)
- Searched for 2025-2026 papers and added 3 new entries to bibliography:
  - Omniwise (arXiv 2025): LLM-based GPU kernel performance prediction
  - HERMES (arXiv 2025): Multi-stage AI inference pipeline simulator
  - POD-Attention (ASPLOS 2025): Prefill-decode overlap for LLM inference
- Updated data/papers/llm-inference.md with new papers (now 45 total)

### Key Findings from Search
- LLM-based performance prediction emerging (Omniwise uses LLaMA for GPU kernel prediction)
- Multi-stage inference simulation gaining traction (HERMES models RAG, KV retrieval, reasoning)
- Prefill-decode optimization continues as major theme (POD-Attention, DistServe, Sarathi)
- ML reproducibility remains a community challenge (ML Reproducibility Challenge 2025)

### For Next Cycle
- Watch for ISCA 2026 and MICRO 2026 papers when announced
- Could help document Leo's reproducibility findings in structured format
- Consider searching for benchmark datasets used by evaluation tools
