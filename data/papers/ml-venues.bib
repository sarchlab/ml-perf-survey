% ML Venues Bibliography
% Papers on ML performance modeling from MLSys, NeurIPS, ICML, ICLR, OSDI (2018-2024)
% Generated by Maya (Literature Scout)

% --- Foundational/Systems Papers ---

@inproceedings{tvm2018,
  author    = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and others},
  title     = {TVM: An Automated End-to-End Optimizing Compiler for Deep Learning},
  booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year      = {2018},
  pages     = {578--594},
  note      = {Foundational deep learning compiler with learned cost model.}
}

@inproceedings{halide-autoscheduler2019,
  author    = {Adams, Andrew and Ma, Karima and Anderson, Luke and Baghdadi, Riyadh and Li, Tzu-Mao and Gharbi, Micha{\"e}l and Steiner, Benoit and Johnson, Steven and Fatahalian, Kayvon and Durand, Fr{\'e}do and Ragan-Kelley, Jonathan},
  title     = {Learning to Optimize Halide with Tree Search and Random Programs},
  booktitle = {ACM SIGGRAPH},
  year      = {2019},
  doi       = {10.1145/3306346.3322967},
  note      = {Learned cost model for image processing pipeline scheduling.}
}

% --- 2020 ---

@inproceedings{ansor2020,
  author    = {Zheng, Lianmin and Jia, Chengfan and Sun, Minmin and Wu, Zhao and Yu, Cody Hao and Haj-Ali, Ameer and Wang, Yida and Yang, Jun and Zhuo, Danyang and Sen, Koushik and Gonzalez, Joseph E. and Stoica, Ion},
  title     = {Ansor: Generating High-Performance Tensor Programs for Deep Learning},
  booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year      = {2020},
  pages     = {863--879},
  note      = {Hierarchical search with learned cost model. Up to 3.8x speedup.}
}

@inproceedings{ofa2020,
  author    = {Cai, Han and Gan, Chuang and Wang, Tianzhe and Zhang, Zhekai and Han, Song},
  title     = {Once-for-All: Train One Network and Specialize it for Efficient Deployment},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2020},
  note      = {Efficient NAS with accuracy predictor. 10 quintillion settings.}
}

% --- 2021 ---

@inproceedings{nnmeter2021,
  author    = {Zhang, Li Lyna and Han, Shihao and Wei, Jianyu and Zheng, Ningxin and Cao, Ting and Yang, Yuqing and Liu, Yunxin},
  title     = {nn-Meter: Towards Accurate Latency Prediction of Deep-Learning Model Inference on Diverse Edge Devices},
  booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services (MobiSys)},
  year      = {2021},
  doi       = {10.1145/3458864.3467882},
  note      = {Best Paper Award. Kernel-level latency prediction, 99\% accuracy.}
}

@inproceedings{tenset2021,
  author    = {Zheng, Lianmin and others},
  title     = {TenSet: A Large-scale Program Performance Dataset for Learned Tensor Compilers},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track},
  year      = {2021},
  note      = {First large public dataset for training cost models in TVM.}
}

% --- 2022 ---

@inproceedings{metaschedule2022,
  author    = {Shao, Junru and others},
  title     = {Tensor Program Optimization with Probabilistic Programs},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2022},
  note      = {MetaSchedule: probabilistic cost-model-driven search.}
}

% --- 2023 ---

@inproceedings{flexgen2023,
  author    = {Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
  title     = {FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning (ICML)},
  year      = {2023},
  note      = {IO-efficient LLM inference with linear programming for offloading.}
}

% --- 2024 ---

@inproceedings{vidur2024,
  author    = {Agrawal, Paras and Panwar, Abhishek and others},
  title     = {VIDUR: A Large-Scale Simulation Framework for LLM Inference},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2024},
  note      = {LLM inference simulator with <5\% error. Open-source.}
}

@inproceedings{latency-predictors2024,
  author    = {various},
  title     = {On Latency Predictors for Neural Architecture Search},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2024},
  note      = {Comprehensive study of NAS latency predictors. 22.5\% improvement.}
}

@inproceedings{flashdecoding2024,
  author    = {various},
  title     = {FlashDecoding++: Faster Large Language Model Inference with Asynchronization, Flat GEMM Optimization, and Heuristics},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2024},
  note      = {LLM inference optimization techniques.}
}

@inproceedings{medusa2024,
  author    = {Cai, Tianle and Li, Yuhong and Geng, Zhengyang and Peng, Hongwu and Lee, Jason D. and Chen, Deming and Dao, Tri},
  title     = {MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning (ICML)},
  year      = {2024},
  note      = {Speculative decoding with multiple heads. 2.2-2.8x speedup.}
}

@inproceedings{hwgptbench2024,
  author    = {various},
  title     = {HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track},
  year      = {2024},
  note      = {Benchmark for hardware-aware NAS for LLMs.}
}

@inproceedings{metatune2021,
  author    = {Janik, Kacper and Dudziak, {\L}ukasz and Lane, Nicholas D.},
  title     = {MetaTune: Meta-Learning Based Cost Model for Fast and Efficient Auto-tuning Frameworks},
  booktitle = {arXiv preprint arXiv:2102.04199},
  year      = {2021},
  note      = {Meta-learning for autotuning cost models.}
}

@inproceedings{roft2025,
  author    = {various},
  title     = {Accelerating the Tuning Process for Optimizing DNN Operators by ROFT Model},
  journal   = {Scientific Reports},
  year      = {2025},
  note      = {Roofline-based cost model for autotuning. 4-10x faster than AutoTVM.}
}

@inproceedings{calognn2024,
  author    = {various},
  title     = {CALO-GNN: Calibrated-Uncertainty Graph Cost Models for Cross-Device TVM Meta-Schedule},
  booktitle = {OpenReview (under review)},
  year      = {2024},
  note      = {First evidential GNN cost model for TVM with calibrated uncertainty.}
}
