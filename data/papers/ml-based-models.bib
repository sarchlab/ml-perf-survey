% ML-Based Performance Modeling Papers
% Focus: Neural network surrogates, GNN-based models, transfer learning, LLM prediction
% Generated by Maya (Literature Scout) for issue #11
% Coverage: 2023-2026

% --- Neural Network Surrogate Models ---

@inproceedings{neusight2025,
  author    = {Yu, Hanming and Hsieh, Tsung-Wei and Zhao, Jerry and Yan, Bohan and Ding, Qian and Cong, Jason and Asanovic, Krste and Pekhimenko, Gennady},
  title     = {NeuSight: Forecasting GPU Performance for Deep Learning Training and Inference},
  booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
  year      = {2025},
  doi       = {10.1145/3669940.3707265},
  note      = {Tile-based GPU performance prediction. Reduces GPT-3 latency error from 121\% to 2.3\% on H100.}
}

@inproceedings{esm2025,
  author    = {various},
  title     = {ESM: A Framework for Building Effective Surrogate Models for Hardware-Aware Neural Architecture Search},
  booktitle = {Design Automation Conference (DAC)},
  year      = {2025},
  note      = {Surrogate model framework for HW-aware NAS. 97.6\% accuracy with FCC encoding.}
}

@inproceedings{litepred2024,
  author    = {Feng, Chengquan and Han, Shihao and Zhang, Ningxin and Cao, Ting and Yang, Yuqing and Liu, Yunxin},
  title     = {LitePred: Transferable and Scalable Latency Prediction for Hardware-Aware Neural Architecture Search},
  booktitle = {21st USENIX Symposium on Networked Systems Design and Implementation (NSDI)},
  year      = {2024},
  url       = {https://www.usenix.org/conference/nsdi24/presentation/feng-chengquan},
  note      = {Transfer learning for latency prediction. 99.3\% accuracy on 85 edge platforms.}
}

@inproceedings{habitat2021,
  author    = {Yu, Geoffrey X. and Gao, Yubo and Golber, Pavel and Pekhimenko, Gennady},
  title     = {Habitat: A Runtime-Based Computational Performance Predictor for Deep Neural Network Training},
  booktitle = {2021 USENIX Annual Technical Conference (ATC)},
  year      = {2021},
  url       = {https://www.usenix.org/conference/atc21/presentation/yu},
  note      = {Wave scaling + MLP for cross-GPU prediction. Foundational work for NeuSight.}
}

% --- GNN-Based Cost Models ---

@inproceedings{tcgnn2022,
  author    = {Wang, Yuke and Feng, Boyuan and Wang, Gushu and Li, Shuai and others},
  title     = {TC-GNN: Accelerating Sparse Graph Neural Network Computation Via Dense Tensor Core on GPUs},
  booktitle = {Proceedings of IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  year      = {2022},
  note      = {First Tensor Core GNN accelerator. 1.70x speedup over DGL.}
}

@inproceedings{gnn-computational-graph2022,
  author    = {Wan, Borui and others},
  title     = {Understanding GNN Computational Graph: A Coordinated Computation, IO, and Memory Perspective},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2022},
  note      = {GNN computational graph analysis. 2.75x speedup, 6.89x less memory IO.}
}

@inproceedings{hypergef2023,
  author    = {Yu, Zhongming and others},
  title     = {HyperGef: A Framework Enabling Efficient Fusion for Hypergraph Neural Network on GPUs},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2023},
  note      = {Kernel fusion for hypergraph neural networks on GPUs.}
}

@inproceedings{gpu-occupancy-gnn2023,
  author    = {various},
  title     = {GPU Occupancy Prediction of Deep Learning Models Using Graph Neural Network},
  booktitle = {IEEE International Conference on Cluster Computing (CLUSTER)},
  year      = {2023},
  note      = {GNN-based GPU occupancy prediction for DNN models.}
}

% --- Transfer Learning for Hardware Prediction ---

@inproceedings{latency-predictors-mlsys2024,
  author    = {Dudziak, {\L}ukasz and Chau, Thomas and Abdelfattah, Mohamed S. and Lee, Royson and Kim, Hyeji and Lane, Nicholas D.},
  title     = {On Latency Predictors for Neural Architecture Search},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2024},
  url       = {https://proceedings.mlsys.org/paper_files/paper/2024/file/f03cb785864596fa5901f1359d23fd81-Paper-Conference.pdf},
  note      = {Comprehensive NAS latency predictor study. 22.5\% improvement with transfer learning.}
}

@article{multi-hardware-latency2024,
  author    = {various},
  title     = {Multi-Hardware Adaptive Latency Prediction for Neural Architecture Search},
  journal   = {arXiv preprint},
  year      = {2024},
  note      = {Cross-platform latency prediction for NAS.}
}

@article{cnn-latency-heterogeneous2024,
  author    = {various},
  title     = {Inference Latency Prediction for CNNs on Heterogeneous Mobile Devices and ML Frameworks},
  journal   = {Performance Evaluation},
  year      = {2024},
  doi       = {10.1016/j.peva.2024.102429},
  note      = {CNN latency prediction across mobile devices and frameworks.}
}

@inproceedings{riscv-nas2024,
  author    = {various},
  title     = {Latency-Constrained Neural Architecture Search Method for Efficient Model Deployment on RISC-V Devices},
  booktitle = {Electronics},
  year      = {2024},
  doi       = {10.3390/electronics13040692},
  note      = {First latency-constrained NAS for RISC-V architecture.}
}

% --- LLM Inference Performance Modeling ---

@inproceedings{roofline-llm2024,
  author    = {Imai, Saki and others},
  title     = {Predicting LLM Inference Latency: A Roofline-Driven ML Method},
  booktitle = {NeurIPS Workshop on Machine Learning for Systems},
  year      = {2024},
  url       = {https://mlforsystems.org/assets/papers/neurips2024/paper28.pdf},
  note      = {Roofline model combined with ML for LLM latency prediction.}
}

@inproceedings{latency-aware-scaling2025,
  author    = {various},
  title     = {Faster and Better LLMs via Latency-Aware Test-Time Scaling},
  booktitle = {Findings of EMNLP},
  year      = {2025},
  note      = {Branch-wise and sequence-wise parallelism for LLM inference.}
}

@article{edge-llm-survey2025,
  author    = {various},
  title     = {Efficient Inference for Edge Large Language Models: A Survey},
  journal   = {Tsinghua Science and Technology},
  year      = {2025},
  doi       = {10.26599/TST.2025.9010166},
  note      = {Survey on speculative decoding and model offloading for edge LLMs.}
}

@article{queueing-llm2025,
  author    = {various},
  title     = {Queueing, Predictions, and Large Language Models: Challenges and Open Problems},
  journal   = {Stochastic Systems},
  year      = {2025},
  doi       = {10.1287/stsy.2025.0106},
  note      = {Queueing theory applied to LLM serving performance.}
}

% --- Hybrid Analytical + ML Models ---

@inproceedings{tlp2023,
  author    = {Zhai, Yi and others},
  title     = {TLP: A Deep Learning-based Cost Model for Tensor Program Tuning},
  booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
  year      = {2023},
  note      = {Deep learning cost model for tensor program optimization.}
}

@inproceedings{alcop2023,
  author    = {various},
  title     = {ALCOP: Automatic Load-Compute Pipelining in Deep Learning Compiler for AI-GPUs},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2023},
  note      = {XGBoost cost model with analytical pre-training for autotuning.}
}

@article{forecasting-gpu2024,
  author    = {Isaev, Nikoli and Hsia, Samuel and others},
  title     = {Calculon: Scalable Performance Modeling for Distributed Training},
  journal   = {arXiv preprint},
  year      = {2023},
  note      = {Analytical model for distributed training latency decomposition.}
}

@inproceedings{madmax2024,
  author    = {Hsia, Samuel and others},
  title     = {MAD-Max: Memory-Aware Decomposition for Maximum Efficiency in Distributed Training},
  booktitle = {arXiv preprint},
  year      = {2024},
  note      = {Hybrid analytical model for distributed training optimization.}
}

% --- Transformer Hardware Acceleration with Cost Models ---

@inproceedings{energon2023,
  author    = {Zhou, Yujun and others},
  title     = {Energon: Toward Efficient Acceleration of Transformers Using Dynamic Sparse Attention},
  booktitle = {IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  year      = {2023},
  note      = {Dynamic sparse attention accelerator. 168x speedup over CPU, 8.7x over GPU.}
}

@inproceedings{h3datten2023,
  author    = {various},
  title     = {H3DAtten: Heterogeneous 3D Integrated Attention Accelerator for Vision Transformer},
  booktitle = {IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  year      = {2023},
  note      = {Hybrid analog-digital in-memory computing for ViT.}
}

@inproceedings{adaptor2024,
  author    = {various},
  title     = {ADAPTOR: A Runtime-Adaptive Transformer Neural Network Accelerator on FPGAs},
  booktitle = {arXiv preprint},
  year      = {2024},
  note      = {Runtime-adaptive FPGA accelerator for transformer encoders/decoders.}
}

@inproceedings{primate2024,
  author    = {various},
  title     = {Primate: Accelerating Transformer Models with Dynamic Token Pruning and PIM},
  booktitle = {Conference on Computer-Aided Design (ICCAD)},
  year      = {2024},
  note      = {Processing-in-memory acceleration with dynamic pruning.}
}

@inproceedings{swat2024,
  author    = {various},
  title     = {SWAT: An Efficient Swin Transformer Accelerator Based on FPGA},
  booktitle = {Asia and South Pacific Design Automation Conference (ASP-DAC)},
  year      = {2024},
  doi       = {10.1109/ASP-DAC58780.2024.10473931},
  note      = {FPGA accelerator for Swin Transformer.}
}

@inproceedings{bintrans2024,
  author    = {various},
  title     = {Co-Designing Binarized Transformer and Hardware Accelerator for Efficient End-to-End Edge Deployment},
  booktitle = {IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
  year      = {2024},
  doi       = {10.1145/3676536.3676733},
  note      = {Binarized transformer with co-designed hardware accelerator.}
}

% --- Benchmarks and Datasets ---

@article{llm-hw-survey2024,
  author    = {various},
  title     = {Hardware Acceleration of LLMs: A Comprehensive Survey and Comparison},
  journal   = {arXiv preprint arXiv:2409.03384},
  year      = {2024},
  note      = {Survey of LLM hardware acceleration techniques.}
}

@article{transformer-fpga-asic-survey2024,
  author    = {various},
  title     = {A Survey of FPGA and ASIC Designs for Transformer Inference Acceleration and Optimization},
  journal   = {Integration, the VLSI Journal},
  year      = {2024},
  doi       = {10.1016/j.vlsi.2024.102242},
  note      = {Comprehensive survey of transformer hardware accelerators.}
}
