% MICRO 2026 Survey Paper - ML Performance Models
% Template based on IEEE conference format

\documentclass[conference]{IEEEtran}

% Packages
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{multirow}

% Custom commands
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}

\begin{document}

\title{A Survey of Machine Learning Approaches for\\Computer Architecture Performance Modeling}

\author{
\IEEEauthorblockN{Authors TBD}
\IEEEauthorblockA{Affiliations TBD}
}

\maketitle

% ==============================================================================
% ABSTRACT
% ==============================================================================
\begin{abstract}
\todo{Write abstract summarizing the survey scope, methodology, key findings, and contributions. Target: 150-200 words.}
\end{abstract}

\begin{IEEEkeywords}
machine learning, performance modeling, computer architecture, neural networks, survey
\end{IEEEkeywords}

% ==============================================================================
% INTRODUCTION
% ==============================================================================
\section{Introduction}
\label{sec:introduction}

Performance modeling is fundamental to computer architecture research and development.
Architects rely on accurate performance predictions to navigate vast design spaces, optimize hardware-software co-design, and make informed decisions about resource allocation.
Traditional approaches---analytical models~\cite{williams2009roofline} and cycle-accurate simulators~\cite{binkert2011gem5}---have served the community well, but face growing challenges as workloads and hardware become increasingly complex.
Analytical models often oversimplify system behavior, while simulators can require hours or days to evaluate a single design point, making exhaustive exploration impractical.

The rise of deep learning workloads has intensified these challenges.
Modern neural networks exhibit diverse computational patterns---from dense matrix operations in transformers to sparse irregular accesses in graph neural networks---that stress traditional modeling assumptions.
Simultaneously, hardware diversity has exploded: GPUs, TPUs, custom accelerators, and multi-device distributed systems each present unique performance characteristics that resist unified analytical treatment.
This complexity has motivated a new generation of \emph{machine learning-based} performance models that learn predictive functions directly from profiling data.

ML-based performance modeling has emerged as a compelling alternative.
Learned models can capture complex, non-linear relationships between workload characteristics and hardware behavior that elude closed-form analysis.
Recent work demonstrates remarkable accuracy: NeuSight~\cite{neusight2025} achieves 2.3\% error predicting GPT-3 latency on H100 GPUs, while nn-Meter~\cite{nnmeter2021} reaches 99\% accuracy for edge device latency prediction.
Beyond accuracy, these approaches offer practical benefits: models trained on one platform can transfer to new hardware with minimal adaptation~\cite{litepred2024}, and inference-time predictions complete in milliseconds rather than hours.

This survey provides a comprehensive analysis of ML-based performance modeling techniques for computer architecture.
We make the following contributions:
\begin{itemize}
    \item A \textbf{taxonomy} organizing approaches along eight dimensions: modeling technique, target hardware, workload types, prediction targets, accuracy metrics, input requirements, evaluation scope, and reproducibility.
    \item A \textbf{systematic survey} of over 60 papers from architecture venues (MICRO, ISCA, HPCA, ASPLOS) and ML venues (MLSys, NeurIPS, ICML) published between 2016--2025.
    \item A \textbf{comparative analysis} examining trade-offs between accuracy, training cost, generalization, and interpretability across approaches.
    \item An identification of \textbf{open challenges} including data scarcity, cross-platform generalization, and integration with design automation flows.
\end{itemize}

The remainder of this paper is organized as follows.
Section~\ref{sec:background} provides background on traditional performance modeling and relevant ML techniques.
Section~\ref{sec:taxonomy} presents our classification taxonomy.
Section~\ref{sec:survey} surveys approaches organized by target hardware platform.
Section~\ref{sec:comparison} offers comparative analysis across key dimensions.
Section~\ref{sec:challenges} discusses open challenges and future directions.
Section~\ref{sec:conclusion} concludes.

% ==============================================================================
% BACKGROUND
% ==============================================================================
\section{Background}
\label{sec:background}

\subsection{Traditional Performance Modeling}
\label{subsec:traditional-modeling}

Performance modeling has traditionally relied on two complementary approaches: analytical models and cycle-accurate simulation.
This section reviews both paradigms and their limitations, motivating the emergence of ML-based alternatives.

\subsubsection{Analytical Models}

Analytical models express performance as closed-form functions of hardware and workload parameters.
The roofline model~\cite{williams2009roofline} exemplifies this approach, bounding attainable performance by peak compute throughput and memory bandwidth.
Given operational intensity $I$ (FLOP/byte), the roofline predicts performance as $P = \min(\pi, \beta \cdot I)$, where $\pi$ is peak FLOPS and $\beta$ is memory bandwidth.
Despite its simplicity, roofline reasoning guides optimization by revealing compute-bound versus memory-bound regimes.

For DNN accelerators, analytical cost models have become standard practice.
Timeloop~\cite{timeloop2019} models data movement across memory hierarchies for any given mapping (loop order and tiling), computing access counts and energy from architectural parameters.
MAESTRO~\cite{maestro2019} provides a data-centric framework that derives performance from dataflow descriptions.
Sparseloop~\cite{sparseloop2022} extends this methodology to sparse tensor operations, achieving 2000$\times$ speedup over RTL simulation while maintaining accuracy.

Analytical models offer several advantages: fast evaluation (microseconds per design point), interpretability (designers can trace predictions to specific terms), and extrapolation to unseen configurations.
However, they require manual derivation for each target architecture, struggle to capture complex microarchitectural effects (contention, pipeline stalls, caching behavior), and may oversimplify non-linear interactions.

\subsubsection{Cycle-Accurate Simulation}

Cycle-accurate simulators model hardware at register-transfer level, faithfully reproducing timing behavior.
General-purpose simulators like gem5~\cite{binkert2011gem5} support flexible configuration of CPU cores, caches, memory controllers, and interconnects.
For GPUs, simulators such as GPGPU-Sim~\cite{gpgpusim2009} and Accel-Sim~\cite{accelsim2020} model SIMT execution, warp scheduling, and memory coalescing.

Cycle-accurate simulation achieves high fidelity---typically within 5--15\% of real hardware~\cite{binkert2011gem5}---and supports detailed microarchitectural studies.
However, simulation speed presents a fundamental limitation: evaluating a single ResNet-50 inference may require hours, making design space exploration impractical.
ASTRA-sim~\cite{astrasim2023} addresses distributed training at scale through analytical abstractions, but even coarse-grained simulation struggles with the combinatorial explosion of modern ML workloads and hardware configurations.

\subsubsection{The Modeling Gap}

Neither approach fully addresses modern performance modeling needs.
Analytical models are fast but imprecise for complex microarchitectures.
Simulators are accurate but too slow for iterative design.
This tension has intensified as ML workloads diversify (from CNNs to transformers to mixture-of-experts models) and hardware specializes (GPUs, TPUs, custom accelerators).
ML-based performance models offer a middle path: learning complex relationships from profiling data while enabling millisecond-scale inference.

\subsection{Machine Learning Fundamentals}
\label{subsec:ml-fundamentals}

This section provides a brief primer on ML techniques frequently employed in performance modeling, establishing terminology used throughout the survey.

\subsubsection{Classical Machine Learning}

Linear regression and its regularized variants (ridge, LASSO) remain widely used for performance prediction due to their simplicity and interpretability.
Given feature vector $\mathbf{x}$ (e.g., operator parameters, hardware counters), linear models predict $\hat{y} = \mathbf{w}^\top \mathbf{x} + b$.
While unable to capture non-linear relationships, linear models provide baselines and feature importance rankings.

Tree-based ensembles---random forests and gradient boosted trees (XGBoost, LightGBM)---handle non-linearities through recursive partitioning.
These methods dominate when training data is limited ($<$10K samples) and features are well-engineered, often outperforming deep learning in low-data regimes~\cite{nnmeter2021}.

\subsubsection{Deep Learning}

Multi-layer perceptrons (MLPs) learn hierarchical feature representations through stacked non-linear transformations: $\mathbf{h}_{i+1} = \sigma(\mathbf{W}_i \mathbf{h}_i + \mathbf{b}_i)$.
MLPs require minimal feature engineering but need sufficient training data and careful regularization to avoid overfitting.

Recurrent neural networks (RNNs) and their gated variants (LSTM, GRU) process sequential inputs, making them suitable for modeling operator sequences in neural network execution graphs.
However, sequential processing limits parallelization and can miss long-range dependencies.

\subsubsection{Graph Neural Networks}

Graph neural networks (GNNs) operate on graph-structured data through message passing.
For a node $v$ with features $\mathbf{h}_v$, GNNs iteratively update representations by aggregating information from neighbors $\mathcal{N}(v)$:
\begin{equation}
\mathbf{h}_v^{(k+1)} = \phi\left(\mathbf{h}_v^{(k)}, \bigoplus_{u \in \mathcal{N}(v)} \psi(\mathbf{h}_u^{(k)}, \mathbf{e}_{uv})\right)
\end{equation}
where $\phi$ and $\psi$ are learnable functions and $\oplus$ is a permutation-invariant aggregation (sum, mean, max).

GNNs are particularly appealing for performance modeling because DNN computation graphs have natural graph structure.
Nodes represent operators with features (type, parameters), edges represent data dependencies with features (tensor shapes, datatypes).
GNNs can learn to propagate performance-relevant information along these dependencies~\cite{granite2022}.

\subsubsection{Attention and Transformers}

Attention mechanisms compute weighted combinations over input elements, with weights determined by learned compatibility functions.
Self-attention allows each position to attend to all other positions:
\begin{equation}
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\right)\mathbf{V}
\end{equation}

Transformers stack self-attention with feedforward networks, enabling long-range dependency modeling without sequential processing.
Recent performance models leverage transformer architectures to capture complex inter-operator interactions across entire computation graphs.

\subsubsection{Transfer Learning}

Transfer learning adapts models trained on one domain (source) to perform well on another (target).
In performance modeling, this enables training on easily-profiled hardware and transferring to new platforms with limited data.
Common approaches include fine-tuning (adjusting pre-trained weights with target data), domain adaptation (learning domain-invariant representations), and meta-learning (learning to adapt quickly from few examples)~\cite{litepred2024}.

\subsection{Problem Formulation}
\label{subsec:problem-formulation}

We now formally define the performance modeling problem and establish the evaluation framework used throughout this survey.

\subsubsection{Inputs and Outputs}

Performance modeling maps workload and hardware descriptions to performance metrics.
Formally, given workload specification $\mathcal{W}$ and hardware configuration $\mathcal{H}$, a performance model $f$ predicts metric $y$:
\begin{equation}
\hat{y} = f(\mathcal{W}, \mathcal{H}; \theta)
\end{equation}
where $\theta$ represents model parameters (weights for ML models, equations for analytical models).

\textbf{Workload representations} vary by granularity and abstraction:
\begin{itemize}
    \item \emph{Operator-level}: Individual layer parameters (kernel size, channels, batch size)
    \item \emph{Graph-level}: Full computation graph with node and edge features
    \item \emph{IR-level}: Intermediate representations from compilers (TVM~\cite{tvm2018}, XLA)
    \item \emph{Trace-level}: Execution traces capturing runtime behavior
\end{itemize}

\textbf{Hardware representations} similarly span multiple levels:
\begin{itemize}
    \item \emph{Specification}: Static parameters (core count, memory size, bandwidth)
    \item \emph{Counter-based}: Runtime performance counters (cache misses, stalls)
    \item \emph{Embedding}: Learned dense representations of hardware platforms
\end{itemize}

\subsubsection{Prediction Targets}

Performance models target various metrics depending on application requirements:

\textbf{Latency} measures execution time, typically end-to-end inference time or per-layer latency.
Latency prediction is critical for real-time applications with strict deadlines and for optimizing user-facing services.

\textbf{Throughput} captures sustained processing rate: samples per second for inference, tokens per second for language models, or images per second for training.
Throughput optimization maximizes hardware utilization for batch processing.

\textbf{Energy} encompasses power consumption (Watts) and energy per operation (Joules/inference).
Energy prediction is essential for mobile deployment, data center cost optimization, and sustainability considerations.

\textbf{Memory} includes peak memory footprint (for feasibility checking), memory bandwidth utilization, and memory access patterns.

\textbf{Multi-objective} formulations jointly predict multiple metrics, enabling Pareto-optimal design selection balancing latency, energy, and accuracy.

\subsubsection{Accuracy Metrics}

The field employs several accuracy metrics, each with distinct interpretations:

\textbf{Mean Absolute Percentage Error (MAPE)} measures average relative deviation:
\begin{equation}
\text{MAPE} = \frac{100\%}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|
\end{equation}
MAPE is scale-invariant and interpretable (5\% MAPE means predictions typically differ by 5\% from ground truth).

\textbf{Root Mean Square Error (RMSE)} penalizes large errors more heavily:
\begin{equation}
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
\end{equation}

\textbf{Correlation coefficients} (Pearson, Spearman) measure how well predictions track relative ordering---important when models guide design space exploration.

\textbf{Ranking accuracy} directly evaluates whether models correctly order configurations, often measured via Kendall's $\tau$ or top-$k$ accuracy.

\subsubsection{Hardware Targets}

Modern performance modeling spans diverse hardware platforms:

\textbf{CPUs} remain important for general-purpose inference and training of smaller models.
CPU modeling must account for complex cache hierarchies, branch prediction, out-of-order execution, and SIMD vectorization.

\textbf{GPUs} dominate ML training and large-scale inference.
GPU modeling addresses SIMT execution, warp scheduling, memory coalescing, and multi-GPU scaling.

\textbf{TPUs and custom accelerators} employ specialized dataflows for matrix operations.
Modeling these devices requires understanding systolic arrays, on-chip memory hierarchies, and dataflow mappings.

\textbf{Edge devices} (mobile SoCs, embedded NPUs) impose strict power and memory constraints.
Edge modeling emphasizes latency under thermal throttling and memory-limited execution.

\textbf{Distributed systems} scale training across multiple devices and nodes.
Distributed modeling must capture communication overhead, synchronization barriers, and pipeline parallelism.

This diversity of targets, workloads, and metrics motivates our comprehensive taxonomy in Section~\ref{sec:taxonomy}.

% ==============================================================================
% TAXONOMY
% ==============================================================================
\section{Taxonomy}
\label{sec:taxonomy}

We organize the surveyed literature along three primary dimensions: the hardware target being modeled, the machine learning techniques employed, and the input representations used.
Figure~\ref{fig:taxonomy-overview} illustrates how these dimensions intersect to characterize different performance modeling approaches.
This taxonomy extends existing classifications~\cite{timeloop2019,maestro2019} by incorporating the emerging diversity of ML-based methods and their distinctive design choices.

Our classification scheme serves two purposes.
First, it provides a systematic framework for understanding the design space of ML-based performance models---researchers can identify which combinations of targets, techniques, and representations have been explored versus those that remain open.
Second, it enables practitioners to select appropriate methods for their use cases by matching problem characteristics (target hardware, available data, accuracy requirements) to model capabilities.

\begin{figure}[t]
\centering
\small
\begin{tabular}{|l|l|}
\hline
\textbf{Dimension} & \textbf{Categories} \\
\hline
\multirow{5}{*}{Target Hardware} & CPU \\
& GPU \\
& DNN Accelerators (TPU, NPU) \\
& Edge/Mobile Devices \\
& Distributed Systems \\
\hline
\multirow{4}{*}{ML Technique} & Classical ML (RF, XGBoost) \\
& Deep Learning (MLP, RNN) \\
& Graph Neural Networks \\
& Hybrid Analytical+ML \\
\hline
\multirow{4}{*}{Input Representation} & Static Features \\
& Hardware Counters \\
& Graph Representations \\
& Learned Embeddings \\
\hline
\end{tabular}
\caption{Overview of our three-dimensional taxonomy for organizing ML-based performance modeling approaches.}
\label{fig:taxonomy-overview}
\end{figure}

\subsection{By Modeling Target}
\label{subsec:by-target}

The choice of hardware target fundamentally shapes model design, as different platforms exhibit distinct performance characteristics and modeling challenges.

\subsubsection{CPU Performance Modeling}

CPUs present complex modeling challenges due to deep out-of-order pipelines, sophisticated cache hierarchies, and branch prediction.
ML models for CPU performance must capture instruction-level parallelism, cache behavior, and memory access patterns.
Traditional approaches relied on microbenchmark-based linear regression~\cite{binkert2011gem5}, while recent work employs graph neural networks to model basic block throughput~\cite{granite2022}.
CPU modeling remains challenging due to the diversity of microarchitectures and the difficulty of capturing dynamic effects like branch misprediction and cache contention.

\subsubsection{GPU Performance Modeling}

GPUs dominate modern ML training and inference, making accurate GPU performance prediction critical.
GPU modeling must account for SIMT execution, warp scheduling, memory coalescing, and memory bandwidth limitations.
Early approaches used analytical roofline models~\cite{williams2009roofline}, but these struggle with the complex memory hierarchies and occupancy effects of modern GPUs.

ML-based GPU models have achieved remarkable accuracy.
NeuSight~\cite{neusight2025} introduces tile-based prediction that mirrors CUDA's execution model, achieving 2.3\% error on GPT-3 inference across H100, A100, and V100 GPUs.
Habitat~\cite{habitat2021} pioneered runtime-based cross-GPU prediction using wave scaling analysis.
These approaches demonstrate that learned models can capture GPU performance characteristics that elude analytical treatment.

\subsubsection{DNN Accelerator Modeling}

Custom DNN accelerators---including TPUs, NPUs, and systolic array designs---employ specialized dataflows optimized for matrix operations.
Modeling these devices requires understanding the interaction between dataflow, memory hierarchy, and tensor tiling.

Analytical frameworks like Timeloop~\cite{timeloop2019} and MAESTRO~\cite{maestro2019} provide systematic approaches for accelerator design space exploration.
Timeloop models data movement and compute utilization for any valid mapping of operations to hardware, achieving 5--10\% accuracy versus RTL simulation at 2000$\times$ speedup.
MAESTRO offers a data-centric perspective using intuitive dataflow directives.
Sparseloop~\cite{sparseloop2022} extends these frameworks to sparse tensor operations, critical for efficient transformer inference.

ML-based approaches complement analytical models by learning residual corrections or capturing effects not modeled analytically.
ArchGym~\cite{archgym2023} demonstrates that ML surrogate models can achieve 0.61\% RMSE while providing 2000$\times$ speedup over simulation, enabling rapid design space exploration for accelerator development.

\subsubsection{Edge and Mobile Device Modeling}

Edge devices impose strict power, memory, and latency constraints, making accurate prediction essential for deploying ML models on mobile phones, IoT devices, and embedded systems.
The diversity of edge hardware---spanning mobile CPUs, mobile GPUs, NPUs, and DSPs---creates significant challenges for cross-platform prediction.

nn-Meter~\cite{nnmeter2021} addresses this challenge through kernel-level prediction with adaptive sampling, achieving 99\% accuracy across mobile CPUs, GPUs, and Intel VPUs.
LitePred~\cite{litepred2024} extends this work with transfer learning, achieving 99.3\% accuracy across 85 edge platforms with less than one hour of adaptation per new device.
These results demonstrate that ML models can effectively generalize across the heterogeneous edge hardware landscape.

\subsubsection{Distributed System Modeling}

Multi-GPU and multi-node systems introduce communication overhead, synchronization barriers, and parallelism strategy choices that fundamentally change performance characteristics.
Distributed training performance depends on the interplay between compute, memory bandwidth, and network communication.

ASTRA-sim~\cite{astrasim2023} provides end-to-end distributed training simulation, modeling collective communication algorithms, network topology, and compute-communication overlap.
VIDUR~\cite{vidur2024} focuses specifically on LLM inference serving, capturing the unique characteristics of prefill and decode phases, KV cache management, and request scheduling.
These simulation frameworks achieve 5--15\% accuracy versus real clusters while enabling exploration of parallelization strategies at scale.

\subsection{By ML Technique}
\label{subsec:by-technique}

The choice of ML technique reflects trade-offs between accuracy, data efficiency, interpretability, and generalization capability.

\subsubsection{Classical Machine Learning}

Tree-based ensembles---random forests and gradient boosted trees (XGBoost, LightGBM)---remain highly effective for performance modeling, particularly in low-data regimes.
These methods handle non-linear relationships through recursive partitioning, provide feature importance rankings for interpretability, and require minimal hyperparameter tuning.

Classical ML models dominate when training data is limited ($<$10K samples) or when features are well-engineered.
nn-Meter~\cite{nnmeter2021} demonstrates that random forests achieve competitive accuracy with careful kernel-level feature engineering.
The ALCOP framework combines XGBoost with analytical pre-training, using analytical model predictions as features to accelerate autotuning convergence.

\subsubsection{Deep Learning}

Multi-layer perceptrons (MLPs) learn hierarchical feature representations without manual feature engineering.
MLPs are widely used as the prediction head in more complex architectures and as standalone models when sufficient training data is available.
NeuSight~\cite{neusight2025} uses MLPs to predict tile-level GPU utilization, learning complex interactions between tile parameters and hardware characteristics.

Recurrent neural networks (RNNs and LSTMs) process sequential inputs, making them suitable for modeling operator sequences in neural network execution.
However, sequential processing limits parallelization, and attention-based architectures increasingly replace RNNs for sequence modeling tasks.

\subsubsection{Graph Neural Networks}

Graph neural networks (GNNs) have emerged as particularly effective for performance modeling because computational graphs have natural graph structure.
Nodes represent operators with features (type, parameters, shapes), edges represent data dependencies with features (tensor dimensions, datatypes).
GNNs propagate performance-relevant information along these dependencies through message passing.

GRANITE~\cite{granite2022} applies GNNs to basic block throughput estimation, learning to predict CPU performance from instruction dependency graphs.
For DNN workloads, GNN-based models capture inter-operator interactions that flat feature representations miss.
The graph structure also enables natural handling of variable-size networks without padding or truncation.

\subsubsection{Hybrid Analytical+ML Models}

Hybrid approaches combine physics-based analytical models with learned components, achieving both interpretability and high accuracy.
The analytical component provides a strong prior based on hardware characteristics, while the ML component learns residual corrections and complex interactions.

This design philosophy has produced state-of-the-art results.
Analytical pre-training initializes ML models with reasonable predictions, reducing data requirements and improving convergence.
Physics-informed architectures incorporate analytical insights into model structure---NeuSight's tile-based prediction mirrors CUDA's execution model, providing inductive bias that improves generalization.
Residual learning trains ML models to predict the error of analytical models, combining analytical interpretability with ML's ability to capture unmodeled effects.

The latency predictor study~\cite{latencypredictorsnas2024} demonstrates that hybrid approaches with transfer learning achieve 22.5\% average improvement over baselines, with up to 87.6\% improvement on challenging cross-platform prediction tasks.

\subsection{By Input Representation}
\label{subsec:by-input}

Input representation determines what information the model can access and how effectively it can learn performance-relevant patterns.

\subsubsection{Static Features}

Static features derive from workload and hardware specifications without runtime measurement.
For DNN workloads, these include layer parameters (kernel size, channels, stride, batch size), tensor dimensions, and operator types.
Hardware specifications include core counts, memory sizes, bandwidth, and clock frequencies.

Static features enable prediction without profiling, supporting use cases like neural architecture search where thousands of candidate networks must be evaluated.
Feature engineering plays a critical role: effective representations capture computation-to-communication ratios, memory footprint estimates, and parallelization potential.

\subsubsection{Hardware Counters}

Performance counters provide runtime measurements of hardware behavior: cache miss rates, memory bandwidth utilization, instruction throughput, and stall cycles.
Counter-based models can capture dynamic effects invisible to static analysis, including contention, thermal throttling, and runtime scheduling decisions.

The primary limitation is that counter-based models require hardware execution, limiting their applicability for design space exploration or new architecture evaluation.
However, for optimizing existing deployments or debugging performance anomalies, counter-based models provide valuable insights that static approaches cannot match.

\subsubsection{Graph Representations}

Graph representations encode computational graphs with nodes representing operators and edges representing data dependencies.
Node features capture operator characteristics (type, parameters), while edge features encode tensor properties (shape, datatype, memory format).

Graph representations provide several advantages over flat feature vectors: they naturally handle variable-size networks, preserve structural information about operator interactions, and enable permutation-invariant predictions.
GNNs operating on these representations can learn which subgraph patterns indicate performance bottlenecks.

\subsubsection{Learned Embeddings}

Learned embeddings compress high-dimensional or categorical information into dense vector representations.
Hardware embeddings represent diverse devices as points in a learned feature space, enabling transfer learning across platforms.
Operator embeddings capture semantic similarities between operator types that may share performance characteristics.

HELP formulates hardware prediction as meta-learning, learning hardware embeddings that represent devices as black-box functions.
With just 10 measurement samples on a new device, HELP achieves accurate predictions by positioning the device appropriately in the learned embedding space.
This approach is particularly valuable for the fragmented edge hardware landscape, where collecting exhaustive training data for each device is impractical.

Table~\ref{tab:taxonomy-summary} summarizes representative papers across our taxonomy dimensions, illustrating the diversity of approaches and their key characteristics.

\begin{table*}[t]
\centering
\caption{Representative papers classified by our taxonomy dimensions. Accuracy reported as MAPE or correlation where available.}
\label{tab:taxonomy-summary}
\small
\begin{tabular}{llllll}
\toprule
\textbf{Paper} & \textbf{Target} & \textbf{Technique} & \textbf{Input} & \textbf{Accuracy} & \textbf{Key Contribution} \\
\midrule
NeuSight~\cite{neusight2025} & GPU & Hybrid & Static & 2.3\% & Tile-based prediction \\
nn-Meter~\cite{nnmeter2021} & Edge & Classical ML & Static & $<$5\% & Kernel detection \\
LitePred~\cite{litepred2024} & Edge & Transfer & Static & 0.7\% & 85-platform transfer \\
GRANITE~\cite{granite2022} & CPU & GNN & Graph & 0.97 corr & Basic block modeling \\
Timeloop~\cite{timeloop2019} & Accelerator & Analytical & Static & 5--10\% & Loop-nest DSE \\
ASTRA-sim~\cite{astrasim2023} & Distributed & Simulation & Traces & 5--15\% & Collective modeling \\
ArchGym~\cite{archgym2023} & Accelerator & Hybrid & Static & 0.61\% RMSE & ML-aided DSE \\
\bottomrule
\end{tabular}
\end{table*}

% ==============================================================================
% SURVEY OF APPROACHES
% ==============================================================================
\section{Survey of Approaches}
\label{sec:survey}

This section surveys ML-based performance modeling approaches organized by target hardware platform.
For each category, we examine the modeling challenges specific to that platform, describe representative techniques, and synthesize key findings across the literature.
Table~\ref{tab:survey-summary} provides a comprehensive comparison of the surveyed approaches.

\begin{table*}[t]
\centering
\caption{Summary of surveyed ML-based performance modeling approaches, organized by target hardware platform.}
\label{tab:survey-summary}
\small
\begin{tabular}{llllll}
\toprule
\textbf{Paper} & \textbf{Platform} & \textbf{ML Technique} & \textbf{Prediction Target} & \textbf{Error} & \textbf{Key Innovation} \\
\midrule
\multicolumn{6}{l}{\textit{CPU Performance Modeling}} \\
GRANITE~\cite{granite2022} & CPU & GNN & Basic block throughput & 0.97 corr & Instruction graph encoding \\
gem5+ML~\cite{binkert2011gem5} & CPU & Hybrid & Execution time & 10--20\% & Simulation + learning \\
\midrule
\multicolumn{6}{l}{\textit{GPU Performance Modeling}} \\
NeuSight~\cite{neusight2025} & GPU & Hybrid MLP & Kernel/E2E latency & 2.3\% & Tile-based prediction \\
Habitat~\cite{habitat2021} & GPU & MLP & Training time & 11.8\% & Wave scaling analysis \\
Accel-Sim~\cite{accelsim2020} & GPU & Simulation & Cycle-accurate & 10--20\% & SASS trace-driven \\
\midrule
\multicolumn{6}{l}{\textit{DNN Accelerator Modeling}} \\
Timeloop~\cite{timeloop2019} & NPU & Analytical & Latency/Energy & 5--10\% & Loop-nest DSE \\
MAESTRO~\cite{maestro2019} & NPU & Analytical & Latency/Energy & 5--15\% & Data-centric directives \\
Sparseloop~\cite{sparseloop2022} & NPU & Analytical & Sparse tensors & 5--10\% & Compression modeling \\
ArchGym~\cite{archgym2023} & Multi & RL+Surrogate & Multi-objective & 0.61\% & ML-aided DSE \\
\midrule
\multicolumn{6}{l}{\textit{Edge Device Modeling}} \\
nn-Meter~\cite{nnmeter2021} & Edge & RF ensemble & Latency & $<$1\% & Kernel detection \\
LitePred~\cite{litepred2024} & Edge & VAE+MLP & Latency & 0.7\% & 85-platform transfer \\
HELP~\cite{help2021} & Multi & Meta-learning & Latency & 1.9\% & 10-sample adaptation \\
\midrule
\multicolumn{6}{l}{\textit{Distributed and LLM Systems}} \\
ASTRA-sim~\cite{astrasim2023} & Distributed & Simulation & Training time & 5--15\% & Collective modeling \\
VIDUR~\cite{vidur2024} & GPU cluster & Simulation & LLM serving & $<$5\% & Prefill/decode phases \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{CPU Performance Modeling}
\label{subsec:cpu-modeling}

CPU performance modeling for ML workloads presents unique challenges due to complex microarchitectural effects including out-of-order execution, branch prediction, and deep cache hierarchies.
While GPUs have received more attention for DNN training, CPUs remain important for inference---particularly on edge devices and for operators that map poorly to SIMT execution.

\subsubsection{Traditional CPU Performance Modeling}

Traditional CPU modeling relies on cycle-accurate simulation through frameworks like gem5~\cite{binkert2011gem5}.
The gem5 simulator provides multiple fidelity levels: fast functional simulation for correctness validation, and detailed out-of-order models achieving 10--20\% accuracy versus real hardware.
For ML workloads, gem5 extensions such as gem5-Aladdin and SMAUG enable accelerator integration studies.

However, cycle-accurate simulation suffers from fundamental speed limitations---simulating even modest DNN inference requires hours, making design space exploration impractical.
This limitation has motivated ML-based alternatives that learn to predict performance from static program features.

\subsubsection{ML-Based Basic Block Modeling}

GRANITE~\cite{granite2022} represents the state of the art in ML-based CPU performance modeling.
The key insight is that basic block throughput---the steady-state execution rate of a loop body---can be predicted from the instruction dependency graph without simulation.
GRANITE encodes basic blocks as directed graphs where nodes represent instructions with features (opcode, operand types) and edges capture data dependencies.

A graph neural network processes this representation through message passing layers:
\begin{equation}
\mathbf{h}_i^{(k+1)} = \text{MLP}\left(\mathbf{h}_i^{(k)} + \sum_{j \in \mathcal{N}(i)} \mathbf{h}_j^{(k)}\right)
\end{equation}
where $\mathbf{h}_i^{(k)}$ represents instruction $i$'s embedding at layer $k$.
After several message passing rounds, a global pooling operation aggregates instruction embeddings into a single block representation, which a final MLP maps to throughput prediction.

GRANITE achieves 0.97 Kendall's $\tau$ correlation with ground-truth measurements on x86 basic blocks, significantly outperforming prior analytical models like IACA and llvm-mca.
Critically, the learned model generalizes across microarchitectures---a model trained on Skylake transfers to Haswell with only modest accuracy degradation.

\subsubsection{Challenges and Opportunities}

Despite GRANITE's success, several challenges remain for CPU performance modeling.
First, DNN operators often involve memory-bound execution where cache behavior dominates---GRANITE focuses on compute-bound basic blocks and does not model memory hierarchy effects.
Second, modern CPUs feature increasingly complex prefetchers and branch predictors whose behavior is difficult to capture in static features.
Third, CPU-based DNN inference often involves highly optimized library code (Intel MKL, ARM Compute Library) whose performance depends on runtime scheduling decisions.

Hybrid approaches combining coarse-grained simulation with learned correction factors represent a promising direction.
Rather than simulating every cycle, these methods use fast simulation to establish approximate behavior, then train ML models to predict residual errors, potentially achieving simulation accuracy at reduced cost.

\subsection{GPU Performance Modeling}
\label{subsec:gpu-modeling}

GPUs are the dominant platform for ML training and large-scale inference.
Accurate GPU performance prediction is essential for neural architecture search, compiler optimization, and serving system design.
However, GPU performance modeling is challenging due to SIMT execution, complex memory hierarchies, and workload-dependent scheduling behavior.

\subsubsection{Cycle-Accurate GPU Simulation}

GPGPU-Sim~\cite{gpgpusim2009} pioneered detailed GPU simulation, modeling SIMT cores, warp scheduling, memory coalescing, and cache hierarchies.
Accel-Sim~\cite{accelsim2020} extended this foundation with trace-driven simulation and improved correlation with modern GPUs (Turing, Ampere), achieving 0.90--0.97 IPC correlation.

These simulators provide high fidelity---essential for microarchitectural studies---but suffer from 1000--10000$\times$ slowdown versus real GPU execution.
Simulating a single ResNet-50 inference can require hours, making design space exploration impractical.
This has motivated the development of ML-based predictors that achieve comparable accuracy at dramatically reduced cost.

\subsubsection{Learned GPU Performance Models}

Habitat~\cite{habitat2021} introduced \emph{wave scaling} for cross-GPU prediction.
The key insight is that GPU execution time can be decomposed into compute and memory components that scale differently across devices:
\begin{equation}
T_{\text{target}} = T_{\text{compute}} \cdot \frac{P_{\text{source}}}{P_{\text{target}}} + T_{\text{memory}} \cdot \frac{B_{\text{source}}}{B_{\text{target}}}
\end{equation}
where $P$ denotes peak compute throughput and $B$ memory bandwidth.
By profiling on a source GPU and measuring how kernels respond to artificially reduced parallelism (``wave scaling''), Habitat estimates the compute and memory fractions, enabling prediction on unseen target GPUs.

Habitat achieves 11.8\% average error predicting training iteration time across GPU generations (V100 to A100).
However, it requires actual GPU execution for wave scaling measurements and cannot predict performance for unseen models.

NeuSight~\cite{neusight2025} addresses these limitations through \emph{tile-based prediction}.
The key innovation is decomposing GPU kernel execution into tiles---the basic scheduling unit in CUDA---and predicting per-tile behavior:
\begin{equation}
T_{\text{kernel}} = \max_{w \in \text{waves}} \sum_{t \in w} \left( T_{\text{compute}}^{(t)} + T_{\text{memory}}^{(t)} \right)
\end{equation}

This formulation mirrors actual GPU execution semantics: tiles are scheduled in waves, and kernel time is dominated by the slowest wave.
NeuSight uses MLPs to predict tile-level compute and memory times from static features (tile dimensions, register usage, shared memory allocation).

By capturing the wave-level structure, NeuSight achieves remarkable accuracy: 2.3\% error on GPT-3 inference across H100, A100, and V100 GPUs.
This represents a 50$\times$ reduction in error compared to prior approaches like Habitat (121.4\% $\rightarrow$ 2.3\% on H100 for GPT-3).
NeuSight's physics-informed architecture---encoding GPU execution semantics into the model structure---provides strong inductive bias that enables generalization to unseen models and GPUs.

\subsubsection{Compiler Cost Models for GPUs}

The TVM~\cite{tvm2018} and Ansor~\cite{ansor2020} systems use learned cost models to guide tensor program optimization.
Rather than executing every candidate program, XGBoost or MLP models predict execution time from program features (loop bounds, vectorization widths, memory access patterns).

Ansor's hierarchical search combines sketch generation, random annotation, and evolutionary refinement, using the cost model to prune the search space.
With 10K profiled samples, Ansor achieves approximately 15\% MAPE on GPU kernel prediction.
The TenSet dataset provides 52 million program performance records across CPUs and GPUs, enabling pre-trained cost models that accelerate autotuning convergence by 10$\times$.

\subsubsection{LLM Inference Prediction}

Large language model inference presents unique GPU modeling challenges.
LLM execution exhibits distinct \emph{prefill} (compute-bound, parallel prompt processing) and \emph{decode} (memory-bound, sequential token generation) phases with fundamentally different performance characteristics.

VIDUR~\cite{vidur2024} provides discrete-event simulation for LLM serving systems.
Rather than modeling GPU microarchitecture, VIDUR simulates request scheduling, KV cache management, and batching decisions---the system-level factors that dominate serving performance.
VIDUR achieves $<$5\% error on end-to-end serving metrics including time-to-first-token and request latency.

Roofline-LLM extends traditional roofline analysis to LLM inference by decomposing transformer execution into compute-bound (prefill attention, FFN) and memory-bound (decode attention, KV cache access) components.
Combined with learned correction factors, this hybrid approach achieves 87\% reduction in MSE compared to pure roofline predictions.

\subsection{Accelerator Performance Modeling}
\label{subsec:accelerator-modeling}

DNN accelerators---including TPUs, NPUs, and custom ASIC designs---employ specialized dataflows and memory hierarchies optimized for tensor operations.
Modeling these devices requires understanding the interaction between dataflow choices, memory hierarchy utilization, and workload characteristics.

\subsubsection{Analytical Accelerator Modeling}

Timeloop~\cite{timeloop2019} provides the foundational framework for DNN accelerator design space exploration.
The key insight is that accelerator performance can be accurately predicted from loop-nest representations of tensor computations.
For a given architecture specification and mapping (loop order, tiling, spatial distribution), Timeloop analytically computes:

\begin{itemize}
\item \textbf{Data reuse} at each memory level: how many times each tensor element is accessed from each buffer
\item \textbf{Latency}: compute cycles plus memory stall cycles based on bandwidth constraints
\item \textbf{Energy}: access counts multiplied by per-access energy at each memory level
\end{itemize}

Timeloop decouples architecture specification (PEs, buffer sizes, bandwidth) from mapping decisions, enabling systematic exploration of dataflow choices.
The framework achieves 5--10\% accuracy versus RTL simulation while providing 2000$\times$ speedup, making million-point design sweeps tractable.

MAESTRO~\cite{maestro2019} offers a complementary \emph{data-centric} perspective.
Rather than loop-nest transformations, MAESTRO models performance through data movement analysis using compact dataflow directives.
This representation is more intuitive---designers specify how tensors flow through the architecture rather than manipulating loop indices---while achieving comparable accuracy.

Sparseloop~\cite{sparseloop2022} extends analytical modeling to sparse tensor accelerators.
The key challenge is that sparse execution time depends on runtime sparsity patterns, not just static tensor dimensions.
Sparseloop models compression formats (CSR, bitmap, RLE), gating logic, and sparse-dense conversion overhead, enabling accurate prediction for pruned neural networks and sparse attention patterns.

\subsubsection{ML-Augmented Accelerator Design}

ArchGym~\cite{archgym2023} demonstrates how ML-based surrogate models can accelerate accelerator design.
The framework connects ML optimization algorithms (reinforcement learning, Bayesian optimization, evolutionary strategies) to hardware simulators through a unified interface.

A key finding is the \emph{hyperparameter lottery}: ML algorithms show high variance across hyperparameter choices, with optimal settings differing substantially between target designs.
ArchGym addresses this through systematic hyperparameter sweeps enabled by fast surrogate models.
Trained surrogate models achieve 0.61\% RMSE while providing 2000$\times$ speedup over simulation, enabling exploration of hyperparameter configurations that would be intractable with direct simulation.

\subsubsection{FPGA and Emerging Accelerator Modeling}

FPGA-based accelerators present additional modeling challenges due to the flexibility of reconfigurable fabric and the complexity of HLS-generated datapaths.
Recent work applies transfer learning to FPGA design space exploration: models trained on one design can adapt to new architectures with limited additional profiling.

Emerging accelerators---including processing-in-memory (PIM), neuromorphic, and analog compute-in-memory designs---remain underexplored.
These platforms exhibit fundamentally different performance characteristics (energy-dominated by activations, analog noise effects, sparse event-driven computation) that existing frameworks do not address.
Developing unified modeling approaches for this diverse hardware landscape represents an important open challenge.

\subsection{Memory System Modeling}
\label{subsec:memory-modeling}

Memory system behavior increasingly dominates ML workload performance.
Large language models may require hundreds of gigabytes for weights and KV cache, while training workloads stress memory bandwidth through gradient communication.
Accurate memory modeling is essential for understanding performance across the modern hardware landscape.

\subsubsection{Cache and Memory Hierarchy Modeling}

Traditional memory system modeling relies on cache simulation within frameworks like gem5~\cite{binkert2011gem5} and GPGPU-Sim~\cite{gpgpusim2009}.
These simulators model replacement policies, bank conflicts, memory coalescing (for GPUs), and DRAM controller behavior with high fidelity.

For DNN workloads, memory access patterns are often highly regular---streaming through weight and activation tensors---making analytical prediction feasible.
Timeloop~\cite{timeloop2019} models memory hierarchy through data reuse analysis: given a tiling and loop order, the framework computes exact access counts at each memory level.
This analytical approach achieves high accuracy for regular workloads but may miss dynamic effects like cache contention in multi-tenant scenarios.

\subsubsection{KV Cache for LLM Inference}

KV cache management has emerged as the dominant memory challenge for LLM serving.
The attention mechanism requires storing key-value tensors for all previously generated tokens, with memory growing linearly with sequence length and batch size.
For long-context models serving concurrent requests, KV cache can consume hundreds of gigabytes.

vLLM's PagedAttention introduces virtual memory concepts to KV cache management.
By storing KV blocks in non-contiguous physical memory with page tables for address translation, PagedAttention achieves near-zero memory waste from fragmentation.
This system-level optimization yields 2--4$\times$ throughput improvement over prior approaches.

VIDUR~\cite{vidur2024} models KV cache behavior at the serving system level, simulating allocation, eviction, and paging decisions that affect request latency.
More recent work explores KV cache compression through quantization (Oaken), sparsity (ALISA), and adaptive token selection (MorphKV), with potential memory savings exceeding 50\%.
Accurate performance models for these compression techniques---predicting the latency-accuracy tradeoff for different compression levels---remain an open challenge.

\subsubsection{Distributed Memory and Communication}

Multi-GPU and multi-node training introduces communication overhead that can dominate performance at scale.
ASTRA-sim~\cite{astrasim2023} provides end-to-end simulation of distributed training, modeling collective communication algorithms (ring, tree, halving-doubling all-reduce), network topology, and compute-communication overlap.

The simulation decomposes collective operations into point-to-point messages, tracks network contention, and models the interaction between computation and communication phases.
ASTRA-sim achieves 5--15\% error versus real multi-GPU clusters, enabling exploration of parallelization strategies (data parallel, model parallel, pipeline parallel) before expensive hardware experiments.

A key insight from distributed training modeling is that communication overhead depends strongly on message granularity and overlap opportunities.
Chunked gradient communication, where gradients are transmitted in pieces overlapped with backward pass computation, can hide communication latency.
Accurate modeling of this overlap---which depends on operator ordering, chunk sizes, and network bandwidth---is essential for predicting distributed training performance.

\subsection{Cross-Platform and Transfer Learning}
\label{subsec:transfer-learning}

The proliferation of hardware platforms---from edge devices to datacenter GPUs to custom accelerators---creates demand for performance models that generalize across configurations.
Training separate models for each target device is impractical given the diversity of the hardware landscape.
Transfer learning and meta-learning approaches address this challenge by learning shared representations that adapt efficiently to new platforms.

\subsubsection{Hardware-Adaptive Latency Prediction}

HELP~\cite{help2021} formulates cross-hardware prediction as meta-learning.
The key insight is that hardware platforms can be treated as ``tasks'' in meta-learning: each device provides a small sample of profiled networks, and the goal is rapid adaptation to new devices.

HELP learns:
\begin{itemize}
\item \textbf{Architecture encoder}: A GNN that embeds neural network architectures into a fixed-dimensional space
\item \textbf{Hardware encoder}: A learned function that represents devices from their profiled samples
\item \textbf{Predictor}: An MLP that maps (architecture, hardware) pairs to latency
\end{itemize}

Using MAML-style~meta-learning, HELP achieves 93.2\% accuracy with just 10 profiled samples on new devices, reaching 98.1\% with 100 samples.
This sample efficiency is critical for the fragmented edge hardware landscape where collecting exhaustive training data for each device type is impractical.

\subsubsection{Transfer Learning at Scale}

LitePred~\cite{litepred2024} scales cross-platform prediction to 85 edge devices---the most comprehensive evaluation to date.
The framework introduces a VAE-based data sampler that intelligently selects which architectures to profile on new devices.
Rather than random sampling, the VAE identifies architectures that are most informative for learning the device's performance characteristics.

With less than one hour of profiling on a new device, LitePred achieves 99.3\% accuracy on held-out architectures.
This combines pre-trained representations from source platforms with efficient adaptation, demonstrating that the cross-platform transfer learning problem is tractable even at scale.

The latency predictors study~\cite{latencypredictorsnas2024} provides a systematic comparison of transfer learning approaches for NAS.
Key findings include:
\begin{itemize}
\item End-to-end training on pooled multi-platform data outperforms sequential fine-tuning
\item Transfer learning provides 22.5\% average improvement over training from scratch
\item Benefits are largest for challenging cross-platform transfers (up to 87.6\% improvement)
\end{itemize}

\subsubsection{Hybrid Analytical-ML Transfer}

Hybrid approaches combine analytical models with learned components to improve transfer efficiency.
SynPerf decomposes GPU kernel execution into pipeline demands (compute, memory, cache) using analytical models, then trains MLPs to capture cross-pipeline interactions.
The analytical decomposition provides physics-based structure that transfers across GPUs, while the learned component captures device-specific effects.

This hybrid architecture achieves 6.1\% kernel-level error and has been applied to guide Triton kernel optimization, demonstrating 1.7$\times$ speedup on generated kernels.
The combination of interpretable analytical structure with learned flexibility represents a promising direction for transferable performance modeling.

\subsubsection{Open Challenges in Transfer Learning}

Despite progress, several challenges remain.
First, most transfer learning work focuses on CNN architectures; transformers and mixture-of-experts models remain underexplored.
Second, transfer across \emph{workload types} (not just hardware) is challenging---models trained on vision networks may not transfer to language models or graph neural networks.
Third, continual learning for performance models---adapting to hardware and software evolution over time---is largely unexplored.

Foundation models for performance prediction represent an emerging opportunity.
Pre-trained on large-scale profiling datasets spanning diverse architectures and hardware, such models could provide strong initialization for any new prediction task.
The TenSet dataset with 52 million records represents a step in this direction, but comprehensive datasets covering the full range of modern workloads and hardware remain to be developed

% ==============================================================================
% COMPARISON AND ANALYSIS
% ==============================================================================
\section{Comparison and Analysis}
\label{sec:comparison}

\subsection{Accuracy vs. Training Cost}
\label{subsec:accuracy-cost}

\todo{Compare prediction accuracy against data collection and training overhead.}

\subsection{Generalization Capabilities}
\label{subsec:generalization}

\todo{Analyze how well models generalize to unseen workloads and configurations.}

\subsection{Interpretability}
\label{subsec:interpretability}

\todo{Discuss model interpretability and insights gained from ML models.}

\todo{Create comparison tables summarizing key papers across multiple dimensions.}

% ==============================================================================
% OPEN CHALLENGES
% ==============================================================================
\section{Open Challenges and Future Directions}
\label{sec:challenges}

\subsection{Data Availability and Quality}
\label{subsec:data-challenges}

\todo{Discuss challenges in collecting training data, benchmark diversity.}

\subsection{Model Generalization}
\label{subsec:generalization-challenges}

\todo{Challenges in generalizing to new architectures and workloads.}

\subsection{Integration with Design Flows}
\label{subsec:integration-challenges}

\todo{Challenges in integrating ML models into architecture exploration workflows.}

\subsection{Emerging Opportunities}
\label{subsec:opportunities}

\todo{Foundation models for architecture, hardware-software co-design.}

% ==============================================================================
% CONCLUSION
% ==============================================================================
\section{Conclusion}
\label{sec:conclusion}

\todo{Summarize key findings and takeaways from the survey.}

\todo{Reiterate the most promising directions for future research.}

% ==============================================================================
% REFERENCES
% ==============================================================================
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
